# TraceML

A simple CLI tool to automatically trace PyTorch training memory usage.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)  
[![GitHub Stars](https://img.shields.io/github/stars/abhinavsriva/trace_ml?style=social)](https://github.com/abhinavsriva/trace_ml/stargazers)

---

## ğŸ’¡ The Problem

Training large machine learning models often feels like a black box. One minute everything's running â€” the next, you're staring at a cryptic `"CUDA out of memory"` error.  

Pinpointing *which* part of the model is consuming too much memory or slowing things down is frustrating and time-consuming. Traditional profiling tools can be overly complex or lack the granularity deep learning developers need.

---

## âœ¨ Why TraceML?

`traceml` is a lightweight CLI tool to instrument your PyTorch training scripts and get real-time, granular insights into:

- System and process-level CPU & RAM usage  
- PyTorch layer-level memory allocation (via `gc`)

All shown live in your terminal â€” no config, no setup, just plug-and-trace.

---

## ğŸ“¦ Installation

```bash
pip install -e .
```

---

## ğŸš€ Usage

```bash
traceml run <your_training_script.py>
```

TraceML wraps your training script and prints memory insights to the terminal as your model trains.

---

## âœ… Current Features

- ğŸ“Š **Live CPU & RAM usage** (System + Current Process)  
- ğŸ” **PyTorch layer-level memory tracking** (via garbage collection)

---

## ğŸ”­ Coming Soon

- ğŸ§© Layer memory tracing via decorators  
- ğŸ¯ Activation & gradient memory usage  
- ğŸ® Live **GPU memory and utilization tracking**  

---

## ğŸ™Œ Contribute & Feedback

Found it useful? Please provide â­ to the repo.  
Got ideas, feedback, or feature requests? I would love to hear from you.

ğŸ“§ Email: [abhinavsriva@gmail.com](mailto:abhinavsriva@gmail.com)
